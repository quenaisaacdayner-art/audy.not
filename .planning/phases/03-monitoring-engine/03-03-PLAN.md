---
phase: 03-monitoring-engine
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/lib/openai/client.ts
autonomous: true

must_haves:
  truths:
    - "System can classify a post as pain_point, recommendation_request, or not_relevant"
    - "Classification includes confidence score (0-100)"
    - "System can generate persona-driven draft replies"
    - "Draft replies are plain text (no markdown)"
  artifacts:
    - path: "src/lib/openai/client.ts"
      provides: "AI classification and reply generation"
      exports: ["classifyPostIntent", "generateDraftReply", "ClassificationResult", "ReplyGenerationResult"]
  key_links:
    - from: "src/lib/openai/client.ts"
      to: "src/lib/validations/mention.ts"
      via: "zodResponseFormat import"
      pattern: "PostIntentSchema|DraftReplySchema"
---

<objective>
Extend the OpenAI client with AI classification and reply generation for Reddit posts.

Purpose: Enable intelligent filtering of Reddit posts by intent and automated generation of helpful, persona-driven draft replies.
Output: Two new functions added to the OpenAI client: classifyPostIntent and generateDraftReply.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-monitoring-engine/03-RESEARCH.md
@.planning/phases/03-monitoring-engine/03-CONTEXT.md

@src/lib/openai/client.ts
@src/lib/validations/product.ts
@src/lib/validations/mention.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add classification function to OpenAI client</name>
  <files>src/lib/openai/client.ts</files>
  <action>
Extend src/lib/openai/client.ts with classifyPostIntent function.

**Add imports:**
```typescript
import { PostIntentSchema, type PostIntent } from '@/lib/validations/mention'
```

**Add result type:**
```typescript
export interface ClassificationResult {
  success: boolean
  data: PostIntent | null
  error?: string
}
```

**Add classifyPostIntent function:**
```typescript
export async function classifyPostIntent(
  title: string,
  content: string,
  productContext: { name: string; description: string; keywords: string[] }
): Promise<ClassificationResult>
```

Implementation:
1. Check if openai is configured (return error if not)
2. Create chat completion with:
   - Model: 'gpt-4o-mini'
   - System prompt:
     * Role: expert at analyzing Reddit posts to identify potential customers
     * Classification options: pain_point, recommendation_request, not_relevant
     * Product context (name, description, keywords)
     * Guidance for confidence scoring:
       - 75%+ for explicit requests or clear pain expressions
       - 50-74% for posts suggesting potential interest
       - <50% for ambiguous cases
     * Note: all pain_point/recommendation_request posts shown to user regardless of confidence
   - User prompt: post title and content
   - response_format: zodResponseFormat(PostIntentSchema, 'post_intent')
3. Extract parsed response
4. Return { success: true, data: parsed } or error

Use system prompt from 03-RESEARCH.md Pattern 3 as reference, adapt per CONTEXT.md decisions (balanced strictness).
  </action>
  <verify>`npx tsc --noEmit` passes</verify>
  <done>classifyPostIntent function returns structured classification with confidence score</done>
</task>

<task type="auto">
  <name>Task 2: Add reply generation function to OpenAI client</name>
  <files>src/lib/openai/client.ts</files>
  <action>
Add generateDraftReply function to src/lib/openai/client.ts.

**Add import:**
```typescript
import { DraftReplySchema, type DraftReply } from '@/lib/validations/mention'
```
(Combine with existing PostIntentSchema import)

**Add result type:**
```typescript
export interface ReplyGenerationResult {
  success: boolean
  data: DraftReply | null
  error?: string
}
```

**Add generateDraftReply function:**
```typescript
export async function generateDraftReply(
  post: { title: string; content: string },
  product: { name: string; description: string; url?: string },
  persona: { expertise?: string | null; tone?: string | null; phrases_to_avoid?: string | null }
): Promise<ReplyGenerationResult>
```

Implementation:
1. Check if openai is configured (return error if not)
2. Calculate adaptive length guidance based on post length:
   - < 200 chars: "Keep your reply concise (2-3 sentences)"
   - 200-500 chars: "Match the moderate length of the post (3-5 sentences)"
   - > 500 chars: "Provide a thoughtful, detailed response matching the depth of the post"
3. Create chat completion with:
   - Model: 'gpt-4o-mini'
   - System prompt:
     * Role: writing a helpful Reddit reply on behalf of someone
     * Include persona details (expertise, tone, phrases_to_avoid) if provided
     * Default tone: "helpful and friendly" if not specified
     * Guidelines:
       1. Adaptive length guidance
       2. Focus on genuinely helping first
       3. Soft product mention at end only if relevant
       4. Plain text only - no markdown, bullets, formatting
       5. Sound like a real person, not marketing
       6. Subtle product mention: "I've had good results with [product]" style
     * Product info (name, description, url)
   - User prompt: post title and content
   - response_format: zodResponseFormat(DraftReplySchema, 'draft_reply')
4. Extract parsed response
5. Return { success: true, data: parsed } or error

Follow 03-CONTEXT.md decisions:
- Adaptive length: match original post
- Soft product mention: solve problem first, mention product naturally at end
- Plain text only
  </action>
  <verify>`npx tsc --noEmit` passes</verify>
  <done>generateDraftReply function returns persona-driven plain text draft</done>
</task>

</tasks>

<verification>
1. [ ] classifyPostIntent function added to src/lib/openai/client.ts
2. [ ] generateDraftReply function added to src/lib/openai/client.ts
3. [ ] Both functions use zodResponseFormat with schemas from validations/mention.ts
4. [ ] Both functions handle missing OpenAI config gracefully
5. [ ] `npx tsc --noEmit` passes
6. [ ] `npm run build` passes
</verification>

<success_criteria>
- classifyPostIntent returns intent (pain_point/recommendation_request/not_relevant) with confidence 0-100
- generateDraftReply returns plain text reply adapted to post length
- Both functions handle errors gracefully with Result pattern
- gpt-4o-mini used for cost efficiency (as established in Phase 2)
</success_criteria>

<output>
After completion, create `.planning/phases/03-monitoring-engine/03-03-SUMMARY.md`
</output>
